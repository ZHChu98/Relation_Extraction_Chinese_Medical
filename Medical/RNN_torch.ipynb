{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_reader\n",
    "SemData = data_reader.read_data_sets('data', padding=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98055"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(SemData.train.sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params():\n",
    "    def __init__(self):\n",
    "        self.n_inputs = 300\n",
    "        self.n_hidden = 150\n",
    "        self.n_class = 15\n",
    "        self.batch_size = 128\n",
    "        self.n_train = 2000\n",
    "        self.n_display = 100\n",
    "        self.test_size = 500\n",
    "\n",
    "params = Params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "        self.n_inputs = params.n_inputs\n",
    "        self.n_hidden = params.n_hidden\n",
    "        self.n_class = params.n_class\n",
    "        self.batch_size = params.batch_size\n",
    "        self.rnn = nn.RNN(self.n_inputs, self.n_hidden)\n",
    "        self.lstm = nn.LSTM(self.n_inputs, self.n_hidden)\n",
    "        self.fc = nn.Linear(self.n_hidden, self.n_class)\n",
    "        self.h0 = torch.randn(1, 1, self.n_hidden).cuda()\n",
    "        self.c0 = torch.randn(1, 1, self.n_hidden).cuda()\n",
    "    \n",
    "    def forward(self, sentences):\n",
    "        for i in range(len(sentences)):\n",
    "            sen_len = sentences[i].shape[0]\n",
    "            sentence = sentences[i].reshape(1, sen_len, self.n_inputs)\n",
    "            sentence = torch.tensor(sentence.transpose(1, 0, 2)).cuda()\n",
    "            #_, h = self.rnn(sentence, self.h0)\n",
    "            _, (h, _) = self.lstm(sentence, (self.h0, self.c0))\n",
    "            h = torch.nn.functional.dropout(h, 0.5)\n",
    "            out = self.fc(h[0])\n",
    "            if i==0:\n",
    "                output = out\n",
    "            else:\n",
    "                output = torch.cat([output, out])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN()\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    start = time.clock()\n",
    "    for step in range(params.n_train):\n",
    "        sentences, labels = SemData.train.next_batch(params.batch_size)\n",
    "        labels = torch.tensor(labels).cuda()\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(sentences)\n",
    "        _, train_golden = torch.max(labels, 1)\n",
    "        loss = criterion(logits, train_golden)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % params.n_display == 0:\n",
    "            _, preds = torch.max(logits, 1)\n",
    "            train_accuracy = torch.sum(preds == train_golden).item() / params.batch_size\n",
    "            \n",
    "            [test_sentences, test_labels] = SemData.test.next_batch(params.test_size)\n",
    "            test_labels = torch.tensor(test_labels).cuda()\n",
    "            \n",
    "            test_logits = model(test_sentences)\n",
    "            _, test_preds = torch.max(test_logits, 1)\n",
    "            _, test_golden = torch.max(test_labels, 1)\n",
    "            test_loss = criterion(test_logits, test_golden)\n",
    "            test_accuracy = torch.sum(test_preds == test_golden).item() / params.test_size\n",
    "            print(\"<step: %d>\" % (step))\n",
    "            print(\"train_accuracy: %.4g %% local_loss: %.8g\" % (train_accuracy*100, loss.item()))\n",
    "            print(\"test_accuracy: %.4g %% total_loss: %.8g\" % (test_accuracy*100, test_loss.item()))\n",
    "    print(\"------------------------------------\")\n",
    "    print(\"training time: \", time.clock()-start, \" s\")\n",
    "\n",
    "    for _ in range(len(SemData.test.labels)//params.test_size):\n",
    "        [test_sentences, test_labels] = SemData.test.next_batch(params.test_size)\n",
    "        test_labels = torch.tensor(test_labels).cuda()\n",
    "        test_logits = model(test_sentences)\n",
    "        _, test_preds = torch.max(test_logits, 1)\n",
    "        _, test_golden = torch.max(test_labels, 1)\n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "        if total_loss == 0:\n",
    "            total_loss = criterion(test_logits, test_golden).item()\n",
    "        else:\n",
    "            total_loss = total_loss + criterion(test_logits, test_golden).item()\n",
    "        total_accuracy = total_accuracy + torch.sum(test_preds == test_golden).item() / params.test_size\n",
    "        total_loss = total_loss / (len(SemData.test.labels)//params.test_size)\n",
    "        total_accuracy = total_accuracy / (len(SemData.test.labels)//params.test_size)\n",
    "    print(\"total_accuracy: %.4g %% total_loss: %.8g\" % (total_accuracy*100, total_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<step: 0>\n",
      "train_accuracy: 1.562 % local_loss: 2.791641\n",
      "test_accuracy: 21.2 % total_loss: 2.6285262\n",
      "<step: 100>\n",
      "train_accuracy: 84.38 % local_loss: 0.67422211\n",
      "test_accuracy: 78.4 % total_loss: 0.81282979\n",
      "<step: 200>\n",
      "train_accuracy: 76.56 % local_loss: 0.90772069\n",
      "test_accuracy: 78.6 % total_loss: 0.819933\n",
      "<step: 300>\n",
      "train_accuracy: 83.59 % local_loss: 0.66814953\n",
      "test_accuracy: 82.6 % total_loss: 0.7123028\n",
      "<step: 400>\n",
      "train_accuracy: 77.34 % local_loss: 0.89495814\n",
      "test_accuracy: 80.4 % total_loss: 0.78801566\n",
      "<step: 500>\n",
      "train_accuracy: 78.91 % local_loss: 0.84024745\n",
      "test_accuracy: 78.8 % total_loss: 0.85520715\n",
      "<step: 600>\n",
      "train_accuracy: 78.91 % local_loss: 0.83275414\n",
      "test_accuracy: 77.6 % total_loss: 0.89844316\n",
      "<step: 700>\n",
      "train_accuracy: 82.81 % local_loss: 0.68820053\n",
      "test_accuracy: 81.4 % total_loss: 0.72525257\n",
      "<step: 800>\n",
      "train_accuracy: 72.66 % local_loss: 0.97764891\n",
      "test_accuracy: 79.6 % total_loss: 0.79711908\n",
      "<step: 900>\n",
      "train_accuracy: 82.03 % local_loss: 0.70341337\n",
      "test_accuracy: 79 % total_loss: 0.80723459\n",
      "<step: 1000>\n",
      "train_accuracy: 81.25 % local_loss: 0.8231343\n",
      "test_accuracy: 79.6 % total_loss: 0.81597972\n",
      "<step: 1100>\n",
      "train_accuracy: 87.5 % local_loss: 0.57228607\n",
      "test_accuracy: 79.6 % total_loss: 0.79613191\n",
      "<step: 1200>\n",
      "train_accuracy: 85.16 % local_loss: 0.69599944\n",
      "test_accuracy: 78.2 % total_loss: 0.81196129\n",
      "<step: 1300>\n",
      "train_accuracy: 81.25 % local_loss: 0.69748497\n",
      "test_accuracy: 80 % total_loss: 0.77741766\n",
      "<step: 1400>\n",
      "train_accuracy: 80.47 % local_loss: 0.80289328\n",
      "test_accuracy: 78.8 % total_loss: 0.82672048\n",
      "<step: 1500>\n",
      "train_accuracy: 75.78 % local_loss: 0.92851794\n",
      "test_accuracy: 81.4 % total_loss: 0.72525239\n",
      "<step: 1600>\n",
      "train_accuracy: 78.12 % local_loss: 0.82798642\n",
      "test_accuracy: 75.8 % total_loss: 0.89576143\n",
      "<step: 1700>\n",
      "train_accuracy: 77.34 % local_loss: 0.88367164\n",
      "test_accuracy: 77.8 % total_loss: 0.82402128\n",
      "<step: 1800>\n",
      "train_accuracy: 74.22 % local_loss: 0.96230584\n",
      "test_accuracy: 79 % total_loss: 0.7846514\n",
      "<step: 1900>\n",
      "train_accuracy: 81.25 % local_loss: 0.74044198\n",
      "test_accuracy: 75.6 % total_loss: 0.88054878\n",
      "------------------------------------\n",
      "training time:  948.3422387999999  s\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'total_accuracy' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-89c9c040f84a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-65cc437ac051>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_golden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mtotal_accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mtotal_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_logits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_golden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mtotal_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_accuracy\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_preds\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtest_golden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'total_accuracy' referenced before assignment"
     ]
    }
   ],
   "source": [
    "model = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_accuracy: 3.752 % total_loss: 0.039092904\n"
     ]
    }
   ],
   "source": [
    "for _ in range(len(SemData.test.labels)//params.test_size):\n",
    "    [test_sentences, test_labels] = SemData.test.next_batch(params.test_size)\n",
    "    test_labels = torch.tensor(test_labels).cuda()\n",
    "    test_logits = model(test_sentences)\n",
    "    _, test_preds = torch.max(test_logits, 1)\n",
    "    _, test_golden = torch.max(test_labels, 1)\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    if total_loss == 0:\n",
    "        total_loss = criterion(test_logits, test_golden).item()\n",
    "    else:\n",
    "        total_loss = total_loss + criterion(test_logits, test_golden).item()\n",
    "    total_accuracy = total_accuracy + torch.sum(test_preds == test_golden).item() / params.test_size\n",
    "    total_loss = total_loss / (len(SemData.test.labels)//params.test_size)\n",
    "    total_accuracy = total_accuracy / (len(SemData.test.labels)//params.test_size)\n",
    "print(\"total_accuracy: %.4g %% total_loss: %.8g\" % (total_accuracy*100, total_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
